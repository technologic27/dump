{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "c:\\users\\analytics\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from runLSTM import buildRunLSTM\n",
    "import numpy as np\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import Flatten\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.externals import joblib\n",
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/03_07_19_lstm_traindata_01_with_labeled_family.pkl\", \"rb\") as input_file:\n",
    "    generated_data = pickle.load(input_file)\n",
    "\n",
    "list_tuples = [x for x in generated_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = buildRunLSTM()\n",
    "X, y, max_features, maxlen, labels, valid_chars = b.read_data(list_tuples, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "encoder = LabelBinarizer()\n",
    "labels = encoder.fit_transform(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BaseEstimator.get_params of LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "dummy_y = np_utils.to_categorical(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'qadars', 'dyre', 'ran', 'locky DGA', 'mirai (rolling window of domains)', 'padcrypt', 'benign', 'cryptowall - static observed list', 'geodo DGA', 'bamital (domains valid ~5 days)', 'shiotob/urlzone/bebloh DGA - not date seeded', 'tinba', 'conficker', 'necurs', 'qakbot (domains valid ~5 days)', 'corebot DGA', 'dircrypt - not date seeded', 'pandabanker (rolling window of domains)', 'chinad (rolling window of domains)', 'Post Tovar GOZ DGA', 'Volatile Cedar / Explosive (not date seeded)', 'unknowndropper (rolling window of domains)', 'gozi (rolling window of domains)', 'feodo', 'tofsee', 'kraken (mix of date and non-date seeded hostnames and domains)', 'murofet (varying date seeds)', 'banjori - not date seeded', 'ramnit - not date seeded', 'fobber_v1', 'necurs (domains valid ~4 days)', 'proslikefan', 'suppobox (-1 to +1 days - time seeded)', 'pykspa_v2_fake', 'unknownjs (rolling window of domains)', 'pushdo DGA', 'simda', 'ranbyus', 'emotet', 'shifu', 'tofsee (rolling window of domains)', 'pykspa_v1', 'pykspa_v2_real', 'suppobox', 'chinad', 'matsnu', 'matsnu DGA', 'cryptolocker', 'gameover', 'tinba DGA', 'nymaim DGA', 'ramnit', 'proslikefan (rolling window of domains)', 'bamital', 'vidro (rolling window of domains)', 'madmax', 'madmax (rolling window of domains)', 'beebone (not date seeded)', 'padcrypt DGA', 'shiotob', 'dyre DGA', 'vawtrak', 'symmi DGA', 'simda (note date seeded)', 'nymaim', 'shifu DGA', 'ramdo (not date seeded)', 'symmi', 'tempedreve', 'vawtrak (not date seeded)', 'pizd (rolling window of domains)', 'bedep (-4 days to today)', 'tinynuke', 'pykspa (varying date seeds)', 'tempedreve (not date seeded)', 'rovnix', 'sisron (rolling window of domains)', 'dromedan (rolling window of domains)', 'virut DGA', 'P2P Gameover Zeus DGA', 'Cryptolocker - Flashback DGA', 'gspy', 'sphinx (rolling window of domains)', 'xshellghost', 'locky', 'blackhole', 'murofet', 'virut', 'dircrypt', 'hesperbot DGA - not date seeded', 'fobber (not date seeded)', 'vidro', 'ccleaner', 'mydoom', 'banjori', 'g01 (rolling window of domains)', 'omexo', 'fobber_v2'}\n",
      "98\n",
      "98\n"
     ]
    }
   ],
   "source": [
    "myset = set(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "#test_data = scaler.transform(X_malicious)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, y_train, y_test, _, label_test = train_test_split(X, y, labels, test_size=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 66, 128)           5120      \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_8 (CuDNNLSTM)     (None, 128)               132096    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 97)                12513     \n",
      "=================================================================\n",
      "Total params: 149,729\n",
      "Trainable params: 149,729\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_8 to have shape (1,) but got array with shape (98,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-bda3bee46f9c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"sparse_categorical_crossentropy\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"adam\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mfinal_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\analytics\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 952\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m    953\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\analytics\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 789\u001b[1;33m                 exception_prefix='target')\n\u001b[0m\u001b[0;32m    790\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[1;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\analytics\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    136\u001b[0m                             \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m                             str(data_shape))\n\u001b[0m\u001b[0;32m    139\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking target: expected dense_8 to have shape (1,) but got array with shape (98,)"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, CuDNNLSTM, RepeatVector, Dropout\n",
    "from keras.models import Model\n",
    "from keras.layers import TimeDistributed\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, input_length=maxlen))\n",
    "model.add(CuDNNLSTM(128))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(98-1, activation='softmax'))\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
    "model.summary()\n",
    "final_data = model.fit(train_data, y_train, batch_size=128, epochs=5, validation_data=(test_data, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training set (images) shape: {shape}\".format(shape=train_data.shape))\n",
    "print(\"Testing set (images) shape: {shape}\".format(shape=test_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {\n",
    " 0: 'Normal',\n",
    " 1: 'DGA',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_X,valid_X,train_ground,valid_ground = train_test_split(train_data,\n",
    "                                                             train_data,\n",
    "                                                             test_size=0.4,\n",
    "                                                             random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.reshape(train_X, (len(train_X), maxlen, 1))\n",
    "valid_X = np.reshape(valid_X, (len(valid_X), maxlen, 1))\n",
    "train_ground = np.reshape(train_ground, (len(train_ground), maxlen, 1))\n",
    "valid_ground = np.reshape(valid_ground, (len(valid_ground), maxlen, 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(input_img):\n",
    "    #encoder\n",
    "    #input = 1 X 59 (wide and thin)\n",
    "\n",
    "    encoded = LSTM(128, return_sequences=True)(input_img)\n",
    "    encoded = LSTM(64, return_sequences=True)(input_img)\n",
    "    encoded = LSTM(32, return_sequences=False)(input_img)\n",
    "\n",
    "\n",
    "    return encoded\n",
    "\n",
    "def decoder(encoded):    \n",
    "    #decoder\n",
    "    decoded = RepeatVector(timesteps)(encoded)\n",
    "    decoded = LSTM(32, return_sequences=True)(decoded)\n",
    "    decoded = LSTM(64, return_sequences=True)(decoded)\n",
    "    decoded = LSTM(n_features, return_sequences=True)(decoded)\n",
    "    \n",
    "    \n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, LSTM, RepeatVector, Dropout\n",
    "from keras.models import Model\n",
    "from keras.layers import TimeDistributed\n",
    "\n",
    "num_classes = 2\n",
    "epochs = 25\n",
    "batch_size = 128\n",
    "timesteps = train_X.shape[1]\n",
    "n_features = train_X.shape[2] \n",
    "\n",
    "\n",
    "#lstm_autoencoder = Sequential()\n",
    "# Encoder\n",
    "#lstm_autoencoder.add(LSTM(encoding_dim, input_shape=(timesteps, n_features), return_sequences=True))\n",
    "#lstm_autoencoder.add(LSTM(hidden_dim, return_sequences=False))\n",
    "#lstm_autoencoder.add(RepeatVector(timesteps))\n",
    "# Decoder\n",
    "#lstm_autoencoder.add(LSTM(hidden_dim, return_sequences=True))\n",
    "#lstm_autoencoder.add(LSTM(hidden_dim, return_sequences=True))\n",
    "#lstm_autoencoder.add(TimeDistributed(Dense(n_features)))\n",
    "\n",
    "#lstm_autoencoder.summary()\n",
    "#lstm_autoencoder.compile(loss='mse', optimizer='adam')\n",
    "input_img = Input(shape=(timesteps, n_features))\n",
    "autoencoder = Model(input_img, decoder(encoder(input_img)))\n",
    "autoencoder.compile(loss='mean_squared_error', optimizer = 'adam')\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder_train = autoencoder.fit(train_X, train_ground, batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(valid_X, valid_ground))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.save_weights('autoencoder.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc(enco):\n",
    "    #flat = Flatten()(enco)\n",
    "    den = Dense(128, activation='relu')(enco)\n",
    "    out = Dense(num_classes, activation='sigmoid')(den)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode = encoder(input_img)\n",
    "full_model = Model(input_img,fc(encode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l1,l2 in zip(full_model.layers[:2],autoencoder.layers[0:2]):\n",
    "    l1.set_weights(l2.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in full_model.layers[0:2]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "# Change the labels from categorical to one-hot encoding\n",
    "train_Y_one_hot = to_categorical(y_train)\n",
    "test_Y_one_hot = to_categorical(y_test)\n",
    "\n",
    "# Display the change for category label using one-hot encoding\n",
    "print('Original label:', y_train[2])\n",
    "print('After conversion to one-hot:', train_Y_one_hot[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_train = full_model.fit(train_X[0:10000], train_label[0:10000], batch_size=64,epochs=10,verbose=1,validation_data=(valid_X[0:1000], valid_label[0:1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set layers trainable to true\n",
    "for layer in full_model.layers[0:2]:\n",
    "    layer.trainable = True\n",
    "    \n",
    "#recompile\n",
    "full_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",metrics=['accuracy'])\n",
    "\n",
    "#retrain\n",
    "classify_train = full_model.fit(train_X[0:10000], train_label[0:10000], batch_size=64,epochs=10,verbose=1,validation_data=(valid_X[0:10000], valid_label[0:10000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X,valid_X,train_label,valid_label = train_test_split(train_data,train_Y_one_hot,test_size=0.2,random_state=13)\n",
    "train_X = np.reshape(train_X, (len(train_X), maxlen, 1))\n",
    "valid_X = np.reshape(valid_X, (len(valid_X), maxlen, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model.predict(valid_X[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_label[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
